{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mightyoctopus/mockup-data-generator/blob/main/w3_d5_mockup_data_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qtscAf_gBKI7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa5c6b8d-f83a-4e69-ecaa-f94b343aea8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m120.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m106.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.2/297.2 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers torch requests bitsandbytes accelerate anthropic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSQagMneBn6H"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from typing import List, Dict, Tuple\n",
        "from datetime import datetime\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from google.colab import drive, userdata\n",
        "from huggingface_hub import login, snapshot_download\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "import torch\n",
        "from anthropic import Anthropic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UI-LjISuDZke"
      },
      "outputs": [],
      "source": [
        "HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "ANTHROPIC_API_KEY = userdata.get(\"ANTHROPIC_API_KEY\")\n",
        "\n",
        "if HF_TOKEN:\n",
        "    login(HF_TOKEN, add_to_git_credential=True)\n",
        "\n",
        "QWEN_MODEL = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
        "CLAUDE_MODEL = \"claude-3-5-haiku-latest\"\n",
        "\n",
        "claude = Anthropic(api_key=ANTHROPIC_API_KEY)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# drive.flush_and_unmount()\n",
        "\n",
        "### Mount FIRST and pick ONE mount point consistently\n",
        "### (Before placing snapshot_download method)\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "id": "IM2vJTXHBVkK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ee29371-8e0a-4142-c1d5-c9d22cc2ddc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bjA9__mLG5Lw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4be2671f-f864-46ac-bca5-de567708e86d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model already exists in cache.\n"
          ]
        }
      ],
      "source": [
        "cache_path = \"/content/drive/MyDrive/models/huggingface_cache\"\n",
        "os.makedirs(cache_path, exist_ok=True)\n",
        "\n",
        "\n",
        "expected_model_dir = os.path.join(cache_path, QWEN_MODEL.replace(\"/\", \"--\"))\n",
        "\n",
        "if not os.path.exists(expected_model_dir):\n",
        "    print(\"Downloading the model...\")\n",
        "    model_path = snapshot_download(\n",
        "        repo_id=QWEN_MODEL,\n",
        "        cache_dir=cache_path,\n",
        "        local_dir=expected_model_dir,\n",
        "        local_dir_use_symlinks=False\n",
        "    )\n",
        "else:\n",
        "    print(\"Model already exists in cache.\")\n",
        "    model_path = expected_model_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nWhr78wDQg-"
      },
      "outputs": [],
      "source": [
        "def invoke_messages(\n",
        "        rows_num: int,\n",
        "        business_category: str,\n",
        "        columns: str,\n",
        "        instruction: str,\n",
        "        ) -> List[Dict[str, str]]:\n",
        "\n",
        "    system_message = \"\"\"\n",
        "        You are a helpful assistant generating synthetic mockup dataset as per\n",
        "        user's request across all types of businesses and sorts.\n",
        "\n",
        "        User's specific request for the data niche, data column types, and all\n",
        "        other details and your job is to create wonderful mockup data for them\n",
        "        to use for their demo apps or develop in a testing environment.\n",
        "    \"\"\".strip()\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "        Generate a synthetic mockup data that fits the following instruction:\n",
        "        - Number of rows: {rows_num}\n",
        "        - Business area: {business_category}\n",
        "        - Columns: {columns}\n",
        "        - Other instruction: {instruction}\n",
        "        ㅡ Make sure to deliver only the markdown content without any additional comments\n",
        "    \"\"\".strip()\n",
        "\n",
        "    system_message = system_message + \"\"\"\n",
        "        In the case of sql file selection as an output, make sure to\n",
        "        contain the full sql file format, including CREATE TABLE command.\n",
        "    \"\"\".strip()\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "\n",
        "    return messages\n",
        "\n",
        "\n",
        "def pass_claude_msg(format: str, content: str) -> Tuple[str, str]:\n",
        "    claude_sys_msg = \"\"\"\n",
        "        You are a helpful assistant, converting generated outputs (done by other model)\n",
        "        into the format of chosen type:\n",
        "\n",
        "        example: csv, sql, or json format.\n",
        "\n",
        "        NOTE: generate the result output that only includes the markdown content\n",
        "        without any addtional comments!\n",
        "    \"\"\".strip()\n",
        "    claude_user_msg = f\"\"\"\n",
        "        Convert the output into the {format} format for the following content:\n",
        "        ----------------------------------------------------------------------\n",
        "        {content}\n",
        "    \"\"\".strip()\n",
        "\n",
        "    return claude_sys_msg, claude_user_msg\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzE_KAl0Hf6E"
      },
      "outputs": [],
      "source": [
        "### Lazy loader\n",
        "model = None\n",
        "tokenizer = None\n",
        "\n",
        "def enable_model():\n",
        "    global model, tokenizer\n",
        "\n",
        "    if tokenizer is None:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    if model is not None:\n",
        "        return model\n",
        "\n",
        "    bnb = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        device_map={\"\": 0},\n",
        "        torch_dtype=\"auto\",\n",
        "        low_cpu_mem_usage=True,\n",
        "        quantization_config=bnb,\n",
        "    ).eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1qbgZ82vgNz"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_output(messages):\n",
        "    enable_model()\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        return_tensors=\"pt\",\n",
        "        return_dict=True, ### IMPORTANT: to get a mapping\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        padding=True,\n",
        "        return_attention_mask=True\n",
        "    ).to(model.device)\n",
        "\n",
        "    # print(inputs)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=400,\n",
        "        temperature=0.2\n",
        "    )\n",
        "\n",
        "    ### Get the length(num of tokens) of the input prompt\n",
        "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "    ### Slice the generated sequence to skip the prompt length\n",
        "    gen_tokens = outputs[0][prompt_len:]\n",
        "\n",
        "    # print(tokenizer.decode(gen_tokens, skip_special_tokens=True))\n",
        "\n",
        "    return gen_tokens\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def launch_claude_api(sys_msg, user_msg):\n",
        "    response = claude.messages.create(\n",
        "        model=CLAUDE_MODEL,\n",
        "        system=sys_msg,\n",
        "        max_tokens=400,\n",
        "        temperature=0.1,\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": user_msg}\n",
        "        ]\n",
        "    )\n",
        "    return response.content[0].text"
      ],
      "metadata": {
        "id": "e2YI9vtaasQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VbfGzo6IjMkW"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio\n",
        "\n",
        "import gradio as gr\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "025JFaFODKx-"
      },
      "outputs": [],
      "source": [
        "###============= Gradio Fucntion =============###\n",
        "\n",
        "def generate_mockup_data(category, num_data_rows, columns, a_instruction,\n",
        "                         progress=gr.Progress(track_tqdm=True)):\n",
        "\n",
        "    progress(0, desc=\"Loading prompts...\")\n",
        "    msg = invoke_messages(category, num_data_rows, columns, a_instruction)\n",
        "\n",
        "    for t in (0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9):\n",
        "        progress(t, desc=\"Generating output...\")\n",
        "        time.sleep(5)\n",
        "\n",
        "    gen_tokens = generate_output(msg)\n",
        "    progress(1.0, desc=\"Initial output generated.\")\n",
        "\n",
        "    return tokenizer.decode(gen_tokens, skip_special_tokens=True)\n",
        "\n",
        "def show_hidden_row():\n",
        "    return gr.update(visible=True)\n",
        "\n",
        "def make_file(btn_sort: str, category: str, content: str):\n",
        "    '''\n",
        "    btn_sort: one of the 3 download buttons -- csv, sql, json download\n",
        "    category: Business cateogry or area that the data is associated with.\n",
        "    content: LLM generated text output to write in a file\n",
        "    '''\n",
        "\n",
        "    sys_msg, user_msg = pass_claude_msg(btn_sort, content)\n",
        "\n",
        "    claude_output = launch_claude_api(sys_msg, user_msg)\n",
        "    # progress(0.9, desc=\"Output has been generated.\")\n",
        "    # print(\"CLAUDE OUTPUT: \", claude_output)\n",
        "\n",
        "\n",
        "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filepath = f\"/tmp/{category}_mockup_{ts}.{btn_sort}\"\n",
        "\n",
        "    with open(filepath, \"w\") as f:\n",
        "        f.write(claude_output)\n",
        "\n",
        "    return filepath\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnTSyfTQjDID",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "4bd69fea-a34e-47b3-8171-1f40bf741972",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://963cc27fba3affe0da.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://963cc27fba3affe0da.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://963cc27fba3affe0da.gradio.live\n"
          ]
        }
      ],
      "source": [
        "from logging import PlaceHolder\n",
        "###============= Gradio UI =============###\n",
        "\n",
        "def render_interface():\n",
        "    custom_css = \"\"\"\n",
        "        .download-btn {\n",
        "            width: 20px;\n",
        "            padding: 6px 16px;\n",
        "        }\n",
        "    \"\"\"\n",
        "\n",
        "    with gr.Blocks(title=\"Mockup Data Generator\", css=\"footer {visibility:hidden}\") as demo:\n",
        "        category = gr.Textbox(\n",
        "            label=\"Business Area/Category\",\n",
        "            placeholder=\"e.g. HR, Sales, Hospitality, Senior Care, E-commerce, Finance\",\n",
        "            )\n",
        "        num_data_rows = gr.Number(\n",
        "            label=\"Number of Rows\",\n",
        "            placeholder=\"Type number...\",\n",
        "            minimum=10,\n",
        "            maximum=50,\n",
        "            step=10,\n",
        "            precision=0\n",
        "            )\n",
        "        columns = gr.Textbox(\n",
        "            label=\"Insert Columns\",\n",
        "            placeholder=\"Comma, separated...\"\n",
        "        )\n",
        "        a_instruction = gr.Textbox(\n",
        "            label=\"Additional Instruction\"\n",
        "            placeholder=\"Any additional instruction. Leave blank if none.\",\n",
        "            lines=5\n",
        "            )\n",
        "        btn = gr.Button(\n",
        "            value=\"Generate\"\n",
        "        )\n",
        "        out = gr.Textbox(label=\"Result shown here.\")\n",
        "\n",
        "\n",
        "        buttons_row = gr.Row(visible=False)\n",
        "\n",
        "        with buttons_row:\n",
        "            btn_csv = gr.DownloadButton(label=\"Download csv\", size=\"md\", elem_classes=[\"download-btn\"])\n",
        "            btn_sql = gr.DownloadButton(label=\"Download sql\", size=\"md\", elem_classes=[\"download-btn\"])\n",
        "            btn_json = gr.DownloadButton(label=\"Download json\", size=\"md\", elem_classes=[\"download-btn\"])\n",
        "\n",
        "\n",
        "        chain = btn.click(\n",
        "            fn=generate_mockup_data,\n",
        "            inputs=[category, num_data_rows, columns, a_instruction],\n",
        "            outputs=out,\n",
        "            queue=True\n",
        "        )\n",
        "\n",
        "        chain = chain.then(\n",
        "            fn=show_hidden_row,\n",
        "            inputs=None,\n",
        "            outputs=buttons_row,\n",
        "        )\n",
        "\n",
        "\n",
        "        btn_csv.click(\n",
        "        lambda category, data: make_file(\"csv\", category, data),\n",
        "        inputs=[category, out],\n",
        "        outputs=btn_csv\n",
        "        )\n",
        "\n",
        "        btn_sql.click(\n",
        "        lambda category, data: make_file(\"sql\", category, data),\n",
        "        inputs=[category, out],\n",
        "        outputs=btn_sql\n",
        "        )\n",
        "\n",
        "        btn_json.click(\n",
        "        lambda category, data: make_file(\"json\", category, data),\n",
        "        inputs=[category, out],\n",
        "        outputs=btn_json\n",
        "        )\n",
        "\n",
        "    return demo\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app = render_interface()\n",
        "    app.queue()\n",
        "    app.launch(share=True, debug=True, inline=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To-Do\n",
        "\n",
        "1. Revise: Eliminate the file type input and let just the 3 buttons be the decesive factor for the user to decide the file type to be downloaded -- DONE\n",
        "\n",
        "\n",
        "2. Deal with the unresponsive looking widgets(all buttons)\n",
        "\n"
      ],
      "metadata": {
        "id": "Wps3tlXX0UxM"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNcL5drm1yRmRcTNHaJRbc3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}